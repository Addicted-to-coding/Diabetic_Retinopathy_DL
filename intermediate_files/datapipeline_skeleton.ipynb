{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datapipeline_skeleton.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f9b-PI_wIfoL",
        "outputId": "c918ce6c-19bc-4d51-ce8e-c9d9a912dd98"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKJUgNNGIWPj",
        "outputId": "c022ada6-7518-446d-e09b-ef3aaca5d6fc"
      },
      "source": [
        "%cd drive/MyDrive/Fall2021/M226/Project/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Fall2021/M226/Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZCvi9r3IW4a",
        "outputId": "7d56ff25-7e75-4f03-f9d3-5fefac550ac6"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CS226.ipynb      models.ipynb  \u001b[0m\u001b[01;34mtest_images\u001b[0m/  \u001b[01;34mtrain_images\u001b[0m/\n",
            "filename.pickle  test.csv      train.csv     \u001b[01;34mtrain_images_resized\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4uJH6-6QjTx"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pickle\n",
        "from torch.utils import data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-aZSV5qx7Ma"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA-l-l3gIOSV"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from os import path"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYa-0KPcOtCL"
      },
      "source": [
        "# Function to resize images to (320, 320)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEyqz7ZwFcj6"
      },
      "source": [
        "def resize_and_save_images(loc, original_dir, saving_dir, height = 320, width = 320):\n",
        "  data = pd.read_csv(loc)\n",
        "  for i in tqdm(range(len(data))):\n",
        "    image_filename = f'{data.iloc[i].id_code.strip()}.png'\n",
        "    file_loc = original_dir + image_filename\n",
        "    saving_loc = saving_dir + image_filename\n",
        "    if not path.exists(saving_loc):\n",
        "      try:\n",
        "        img = cv2.imread(file_loc)\n",
        "        img = cv2.resize(img, (width, height))\n",
        "        cv2.imwrite(saving_loc, img)\n",
        "      except:\n",
        "        unsaved.append(f'{image_filename}')\n",
        "        # print(f'{image_filename}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92BuZccayFKG",
        "outputId": "ce5887c0-ae45-45d0-8741-020061b2e8dd"
      },
      "source": [
        "resize_and_save_images('train.csv', 'train_images/', 'train_images_resized/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3662/3662 [00:19<00:00, 188.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju9T416lIchs"
      },
      "source": [
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JricJ1ScOxiu"
      },
      "source": [
        "# Data pipeline creation\n",
        "### Creating train, valid, test split along with the dataloader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0XQAMUFJrVJ"
      },
      "source": [
        "def get_image_label(data, dir, indices):\n",
        "\n",
        "  X = np.array([np.asarray(Image.open(f'{dir}{data.iloc[i].id_code}.png')) for i in indices])\n",
        "  y = np.array([int(data.iloc[i].diagnosis) for i in indices])\n",
        "\n",
        "  return X,y"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3ZlPNQ-RuZn"
      },
      "source": [
        "def get_dataset(path, dir, train_size = 0.8, valid_size = 0.1):\n",
        "\n",
        "  data = pd.read_csv(path)\n",
        "  arr = [i for i in range(len(data))]\n",
        "  random.shuffle(arr)\n",
        "  train_indices = arr[: int(train_size * len(data)) ]\n",
        "  train_X, train_y = get_image_label(data, dir, train_indices)\n",
        "\n",
        "  valid_indices = arr[int(train_size * len(data)) : int((train_size + valid_size) * len(data))]\n",
        "  valid_X, valid_y = get_image_label(data, dir, valid_indices)\n",
        "\n",
        "  test_indices = arr[int((train_size + valid_size) * len(data)) : ]\n",
        "  test_X, test_y = get_image_label(data, dir, test_indices)\n",
        "\n",
        "\n",
        "  return train_X, train_y, valid_X, valid_y, test_X, test_y"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B8uTa1xPYNI"
      },
      "source": [
        "# Set path to train.csv and train_images_resized folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3RL6t14K-PZ"
      },
      "source": [
        "train_X, train_y, valid_X, valid_y, test_X, test_y = get_dataset(\"train.csv\", 'train_images_resized/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG6wLJyNLXXp",
        "outputId": "98fd29b5-2904-498b-8a2e-200ed152060d"
      },
      "source": [
        "train_X.shape, train_y.shape, valid_X.shape, valid_y.shape, test_X.shape, test_y.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10, 320, 320, 3), (10,), (10, 320, 320, 3), (10,), (10, 320, 320, 3), (10,))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbqinByJPrvk"
      },
      "source": [
        "class DataGenerator(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    super(DataGenerator, self).__init__()\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.length = len(X)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9Wrp4rMZ-lq"
      },
      "source": [
        "train_dataset = DataGenerator(train_X, train_y)\n",
        "train_loader = data.DataLoader(train_dataset, batch_size= 5, shuffle = True)\n",
        "\n",
        "valid_dataset = DataGenerator(valid_X, valid_y)\n",
        "valid_loader = data.DataLoader(valid_dataset, batch_size = 5, shuffle = True)\n",
        "\n",
        "test_dataset = DataGenerator(test_X, test_y)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size = 5, shuffle = True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnsj-LIIOI23",
        "outputId": "627c9fe2-bbfc-475f-9659-38c875e7c27b"
      },
      "source": [
        "for batch, data_batch in enumerate(train_loader):\n",
        "  print(batch)\n",
        "  print(data_batch[0].shape)\n",
        "  print(data_batch[1].shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "torch.Size([5, 320, 320, 3])\n",
            "torch.Size([5])\n",
            "1\n",
            "torch.Size([5, 320, 320, 3])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHnbcGvQlJD"
      },
      "source": [
        "## Training Skeleton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YdGf7uaPicL"
      },
      "source": [
        "def train(model, optimizer, train_loader, valid_loader, epochs = 1, save_interval = 1):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for epoch in range(1, epochs + 1):\n",
        "    train_loss = 0\n",
        "    for batch_idx, data_batch in enumerate(loader):\n",
        "      optimizer.zero_grad()\n",
        "      train_X, train_y = data_batch[0], data_batch[1]\n",
        "      prediction = model(train_X)\n",
        "      loss = __LOSS__(prediction, train_y)\n",
        "      loss.backward()\n",
        "      train_loss += loss.item()\n",
        "      optimizer.step()\n",
        "    avg_loss = train_loss / len(train_loader.dataset)\n",
        "      \n",
        "    if batch_idx % save_interval == 0:\n",
        "      val_loss = __EVALUATE__(model, valid_loader)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}